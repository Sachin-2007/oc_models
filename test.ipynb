{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a30d48a8",
   "metadata": {},
   "source": [
    "# Bounding Box Overlay Dataset Processor\n",
    "\n",
    "A custom LeRobot processor that overlays bounding box annotations onto image observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70614426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Any\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from lerobot.processor.pipeline import ObservationProcessorStep, ProcessorStepRegistry\n",
    "from lerobot.configs.types import PipelineFeatureType, PolicyFeature, FeatureType\n",
    "from lerobot.utils.constants import OBS_IMAGE, OBS_IMAGES\n",
    "\n",
    "\n",
    "@dataclass\n",
    "@ProcessorStepRegistry.register(name=\"bounding_box_overlay_processor\")\n",
    "class BoundingBoxOverlayProcessor(ObservationProcessorStep):\n",
    "    \"\"\"\n",
    "    A processor that overlays bounding boxes onto image observations.\n",
    "    \n",
    "    This processor takes bounding box data and draws it on image observations,\n",
    "    replacing the original images with annotated versions.\n",
    "    \n",
    "    Attributes:\n",
    "        bbox_key: The key in the observation dict where bounding box data is stored.\n",
    "                  Expected format: dict with image keys mapping to list of bboxes.\n",
    "                  Each bbox should be [x1, y1, x2, y2] or [x1, y1, x2, y2, label].\n",
    "        box_color: RGB tuple for bounding box color (default: red).\n",
    "        box_width: Width of the bounding box lines in pixels.\n",
    "        remove_bbox_key: Whether to remove the bbox_key from observations after processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    bbox_key: str = \"bounding_boxes\"\n",
    "    box_color: tuple[int, int, int] = (255, 0, 0)  # Red\n",
    "    box_width: int = 2\n",
    "    remove_bbox_key: bool = True\n",
    "    \n",
    "    def observation(self, obs: dict[str, Any]) -> dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process observation by drawing bounding boxes on images.\n",
    "        \n",
    "        Args:\n",
    "            obs: Observation dictionary containing images and bounding boxes.\n",
    "            \n",
    "        Returns:\n",
    "            Modified observation with bounding boxes drawn on images.\n",
    "        \"\"\"\n",
    "        new_obs = obs.copy()\n",
    "        \n",
    "        # Check if bounding box data exists\n",
    "        if self.bbox_key not in obs:\n",
    "            return new_obs\n",
    "            \n",
    "        bboxes = obs[self.bbox_key]\n",
    "        \n",
    "        # Handle single image case (OBS_IMAGE key)\n",
    "        if OBS_IMAGE in new_obs:\n",
    "            img_data = new_obs[OBS_IMAGE]\n",
    "            if isinstance(bboxes, (list, np.ndarray)):\n",
    "                # Bboxes are for this single image\n",
    "                new_obs[OBS_IMAGE] = self._draw_boxes_on_image(img_data, bboxes)\n",
    "        \n",
    "        # Handle multiple images case (OBS_IMAGES.* keys or dictionary)\n",
    "        if isinstance(bboxes, dict):\n",
    "            for img_key, img_bboxes in bboxes.items():\n",
    "                # Construct the full observation key\n",
    "                full_key = f\"{OBS_IMAGES}.{img_key}\" if not img_key.startswith(OBS_IMAGES) else img_key\n",
    "                \n",
    "                if full_key in new_obs:\n",
    "                    img_data = new_obs[full_key]\n",
    "                    new_obs[full_key] = self._draw_boxes_on_image(img_data, img_bboxes)\n",
    "        \n",
    "        # Remove bounding box data if requested\n",
    "        if self.remove_bbox_key and self.bbox_key in new_obs:\n",
    "            del new_obs[self.bbox_key]\n",
    "            \n",
    "        return new_obs\n",
    "    \n",
    "    def _draw_boxes_on_image(self, img_data: Any, bboxes: list) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Draw bounding boxes on a single image.\n",
    "        \n",
    "        Args:\n",
    "            img_data: Image data (can be torch.Tensor or np.ndarray).\n",
    "            bboxes: List of bounding boxes, each as [x1, y1, x2, y2] or [x1, y1, x2, y2, label].\n",
    "            \n",
    "        Returns:\n",
    "            Annotated image as a torch.Tensor in the same format as input.\n",
    "        \"\"\"\n",
    "        if len(bboxes) == 0:\n",
    "            # No boxes to draw, return original\n",
    "            if isinstance(img_data, torch.Tensor):\n",
    "                return img_data\n",
    "            return torch.from_numpy(img_data)\n",
    "        \n",
    "        # Convert to numpy for PIL processing\n",
    "        if isinstance(img_data, torch.Tensor):\n",
    "            img_np = img_data.cpu().numpy()\n",
    "            was_tensor = True\n",
    "        else:\n",
    "            img_np = img_data\n",
    "            was_tensor = False\n",
    "        \n",
    "        # Handle different tensor formats\n",
    "        # Expected formats: (B, C, H, W) or (C, H, W) or (H, W, C)\n",
    "        original_shape = img_np.shape\n",
    "        is_batched = False\n",
    "        is_normalized = img_np.dtype == np.float32 or img_np.dtype == np.float64\n",
    "        \n",
    "        # Remove batch dimension if present\n",
    "        if len(img_np.shape) == 4:\n",
    "            is_batched = True\n",
    "            img_np = img_np[0]  # Take first image in batch\n",
    "        \n",
    "        # Convert channel-first to channel-last if needed\n",
    "        if len(img_np.shape) == 3 and img_np.shape[0] in [1, 3, 4]:  # Likely (C, H, W)\n",
    "            img_np = np.transpose(img_np, (1, 2, 0))\n",
    "            is_channel_first = True\n",
    "        else:\n",
    "            is_channel_first = False\n",
    "        \n",
    "        # Denormalize if needed\n",
    "        if is_normalized:\n",
    "            img_np = (img_np * 255).astype(np.uint8)\n",
    "        else:\n",
    "            img_np = img_np.astype(np.uint8)\n",
    "        \n",
    "        # Convert to PIL for drawing\n",
    "        if img_np.shape[2] == 1:  # Grayscale\n",
    "            pil_img = Image.fromarray(img_np.squeeze(), mode='L')\n",
    "        else:\n",
    "            pil_img = Image.fromarray(img_np, mode='RGB')\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        draw = ImageDraw.Draw(pil_img)\n",
    "        for bbox in bboxes:\n",
    "            if len(bbox) >= 4:\n",
    "                x1, y1, x2, y2 = bbox[:4]\n",
    "                draw.rectangle([x1, y1, x2, y2], outline=self.box_color, width=self.box_width)\n",
    "                \n",
    "                # Optionally draw label if provided\n",
    "                if len(bbox) > 4:\n",
    "                    label = str(bbox[4])\n",
    "                    draw.text((x1, y1 - 10), label, fill=self.box_color)\n",
    "        \n",
    "        # Convert back to numpy\n",
    "        img_np = np.array(pil_img)\n",
    "        \n",
    "        # Restore grayscale channel dimension if needed\n",
    "        if len(img_np.shape) == 2:\n",
    "            img_np = img_np[:, :, np.newaxis]\n",
    "        \n",
    "        # Normalize back if needed\n",
    "        if is_normalized:\n",
    "            img_np = img_np.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Convert back to channel-first if needed\n",
    "        if is_channel_first:\n",
    "            img_np = np.transpose(img_np, (2, 0, 1))\n",
    "        \n",
    "        # Add batch dimension back if needed\n",
    "        if is_batched:\n",
    "            img_np = img_np[np.newaxis, :]\n",
    "        \n",
    "        # Convert to tensor if original was tensor\n",
    "        if was_tensor:\n",
    "            return torch.from_numpy(img_np)\n",
    "        return torch.from_numpy(img_np)\n",
    "    \n",
    "    def transform_features(self, features: dict[PipelineFeatureType, dict[str, PolicyFeature]]) -> dict[PipelineFeatureType, dict[str, PolicyFeature]]:\n",
    "        \"\"\"\n",
    "        Declare feature transformations - images remain the same shape, bbox data is removed.\n",
    "        \n",
    "        Args:\n",
    "            features: Input feature specifications.\n",
    "            \n",
    "        Returns:\n",
    "            Modified feature specifications (bbox key removed if remove_bbox_key=True).\n",
    "        \"\"\"\n",
    "        new_features = features.copy()\n",
    "        \n",
    "        # Remove bounding box feature if it exists and we're configured to remove it\n",
    "        if self.remove_bbox_key and PipelineFeatureType.OBSERVATION in new_features:\n",
    "            obs_features = new_features[PipelineFeatureType.OBSERVATION].copy()\n",
    "            if self.bbox_key in obs_features:\n",
    "                del obs_features[self.bbox_key]\n",
    "            new_features[PipelineFeatureType.OBSERVATION] = obs_features\n",
    "        \n",
    "        # Image features remain unchanged (same dimensions)\n",
    "        return new_features\n",
    "    \n",
    "    def get_config(self) -> dict[str, Any]:\n",
    "        \"\"\"Return configuration for serialization.\"\"\"\n",
    "        return {\n",
    "            \"bbox_key\": self.bbox_key,\n",
    "            \"box_color\": self.box_color,\n",
    "            \"box_width\": self.box_width,\n",
    "            \"remove_bbox_key\": self.remove_bbox_key,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f5bb0",
   "metadata": {},
   "source": [
    "# Bounding Box Overlay Processor\n",
    "\n",
    "This processor takes bounding box data and overlays it onto image observations, then replaces the original images with the annotated versions.\n",
    "\n",
    "## Features:\n",
    "- Handles single images (`observation.image`) or multiple images (`observation.images.*`)\n",
    "- Supports both batched and unbatched image formats\n",
    "- Works with normalized (0-1) or unnormalized (0-255) images\n",
    "- Automatically handles channel-first (C,H,W) or channel-last (H,W,C) formats\n",
    "- Removes bounding box data from observations after processing (configurable)\n",
    "\n",
    "## Expected Bounding Box Format:\n",
    "- For single image: `{\"bounding_boxes\": [[x1, y1, x2, y2], [x1, y1, x2, y2, label], ...]}`\n",
    "- For multiple images: `{\"bounding_boxes\": {\"camera1\": [[x1, y1, x2, y2], ...], \"camera2\": [...]}}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f7c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Using with multiple cameras\n",
    "observation_multi = {\n",
    "    \"pixels\": {\n",
    "        \"front\": np.random.randint(0, 256, size=(480, 640, 3), dtype=np.uint8),\n",
    "        \"wrist\": np.random.randint(0, 256, size=(480, 640, 3), dtype=np.uint8),\n",
    "    },\n",
    "    \"bounding_boxes\": {\n",
    "        \"front\": [[50, 50, 150, 150], [200, 200, 300, 300]],\n",
    "        \"wrist\": [[100, 100, 200, 200, \"gripper\"]],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Process multiple camera observations\n",
    "transition_multi = create_transition(observation=observation_multi)\n",
    "processed_multi = bbox_processor(transition_multi)\n",
    "\n",
    "print(\"Multi-camera processing complete\")\n",
    "print(\"Keys:\", list(processed_multi[TransitionKey.OBSERVATION].keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c038c",
   "metadata": {},
   "source": [
    "## Usage in Dataset Recording\n",
    "\n",
    "When recording a dataset with bounding box data, you can use this processor to create annotated images:\n",
    "\n",
    "```python\n",
    "from lerobot.datasets.lerobot_dataset import LeRobotDataset\n",
    "from lerobot.datasets.pipeline_features import create_initial_features, aggregate_pipeline_dataset_features\n",
    "\n",
    "# Define your robot's features including bounding boxes\n",
    "initial_features = create_initial_features(\n",
    "    observation={\n",
    "        \"pixels\": (480, 640, 3),  # Camera image\n",
    "        \"bounding_boxes\": list,    # Bounding box data\n",
    "        \"agent_pos\": (7,),         # Robot state\n",
    "    },\n",
    "    action={\"joint_positions\": (7,)}\n",
    ")\n",
    "\n",
    "# Create pipeline with bounding box overlay\n",
    "recording_pipeline = PolicyProcessorPipeline(\n",
    "    steps=[\n",
    "        BoundingBoxOverlayProcessor(bbox_key=\"bounding_boxes\"),\n",
    "        VanillaObservationProcessorStep(),\n",
    "    ],\n",
    "    to_transition=batch_to_transition,\n",
    "    to_output=transition_to_batch\n",
    ")\n",
    "\n",
    "# Get final features after processing\n",
    "final_features = aggregate_pipeline_dataset_features(\n",
    "    pipeline=recording_pipeline,\n",
    "    initial_features=initial_features,\n",
    "    use_videos=True\n",
    ")\n",
    "\n",
    "# Create dataset with processed features\n",
    "dataset = LeRobotDataset.create(\n",
    "    repo_id=\"user/dataset_with_bbox_overlay\",\n",
    "    features=final_features,\n",
    "    # ... other parameters\n",
    ")\n",
    "```\n",
    "\n",
    "The bounding boxes will be drawn on the images before they're saved to the dataset!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ca21b4",
   "metadata": {},
   "source": [
    "## Customization Options\n",
    "\n",
    "You can customize the appearance and behavior of the bounding box overlay:\n",
    "\n",
    "```python\n",
    "# Custom colors and line widths\n",
    "processor_green = BoundingBoxOverlayProcessor(\n",
    "    bbox_key=\"detections\",          # Custom key name\n",
    "    box_color=(0, 255, 0),          # Green boxes\n",
    "    box_width=5,                    # Thicker lines\n",
    "    remove_bbox_key=False           # Keep bbox data in observations\n",
    ")\n",
    "\n",
    "# Multiple colors for different object types (requires custom logic)\n",
    "# You can extend the processor to support different colors per bbox\n",
    "```\n",
    "\n",
    "## Key Features:\n",
    "- ✅ Automatically handles batched/unbatched images\n",
    "- ✅ Works with channel-first (C,H,W) and channel-last (H,W,C) formats\n",
    "- ✅ Preserves normalized (0-1) or denormalized (0-255) ranges\n",
    "- ✅ Supports single or multiple camera setups\n",
    "- ✅ Optional text labels on bounding boxes\n",
    "- ✅ Integrates seamlessly with LeRobot processor pipelines\n",
    "- ✅ Registered in ProcessorStepRegistry for easy configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5d0143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Using in a PolicyProcessorPipeline\n",
    "from lerobot.processor.pipeline import PolicyProcessorPipeline\n",
    "from lerobot.processor import VanillaObservationProcessorStep\n",
    "from lerobot.processor.converters import batch_to_transition, transition_to_batch\n",
    "\n",
    "# Create a pipeline that:\n",
    "# 1. Draws bounding boxes on images\n",
    "# 2. Processes observations (converts to LeRobot format)\n",
    "pipeline = PolicyProcessorPipeline(\n",
    "    steps=[\n",
    "        BoundingBoxOverlayProcessor(bbox_key=\"bounding_boxes\"),\n",
    "        VanillaObservationProcessorStep(),\n",
    "    ],\n",
    "    name=\"bbox_overlay_pipeline\",\n",
    "    to_transition=batch_to_transition,\n",
    "    to_output=transition_to_batch\n",
    ")\n",
    "\n",
    "# Create sample batch data\n",
    "sample_batch = {\n",
    "    \"pixels\": np.random.randint(0, 256, size=(480, 640, 3), dtype=np.uint8),\n",
    "    \"bounding_boxes\": [[100, 150, 200, 250], [300, 300, 400, 400]],\n",
    "    \"agent_pos\": np.array([0.1, 0.2, 0.3]),\n",
    "}\n",
    "\n",
    "# Process through pipeline\n",
    "processed_batch = pipeline(sample_batch)\n",
    "\n",
    "print(\"Pipeline processing complete!\")\n",
    "print(\"Output keys:\", list(processed_batch.keys()))\n",
    "print(\"Has observation.image:\", \"observation.image\" in processed_batch)\n",
    "print(\"Has observation.state:\", \"observation.state\" in processed_batch)\n",
    "print(\"Bounding boxes removed:\", \"bounding_boxes\" not in processed_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Using the processor with a single image\n",
    "from lerobot.processor.converters import create_transition\n",
    "from lerobot.processor import TransitionKey\n",
    "\n",
    "# Create a sample observation with image and bounding boxes\n",
    "image = np.random.randint(0, 256, size=(480, 640, 3), dtype=np.uint8)\n",
    "bboxes = [\n",
    "    [100, 100, 200, 200],  # x1, y1, x2, y2\n",
    "    [300, 300, 400, 450, \"object\"],  # with label\n",
    "]\n",
    "\n",
    "observation = {\n",
    "    \"pixels\": image,\n",
    "    \"bounding_boxes\": bboxes\n",
    "}\n",
    "\n",
    "# Create processor\n",
    "bbox_processor = BoundingBoxOverlayProcessor(\n",
    "    bbox_key=\"bounding_boxes\",\n",
    "    box_color=(255, 0, 0),  # Red\n",
    "    box_width=3\n",
    ")\n",
    "\n",
    "# Process the observation\n",
    "transition = create_transition(observation=observation)\n",
    "processed_transition = bbox_processor(transition)\n",
    "\n",
    "# The processed observation now has bounding boxes drawn on the image\n",
    "processed_obs = processed_transition[TransitionKey.OBSERVATION]\n",
    "print(\"Keys in processed observation:\", list(processed_obs.keys()))\n",
    "print(\"Bounding boxes removed:\", \"bounding_boxes\" not in processed_obs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lerobot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
